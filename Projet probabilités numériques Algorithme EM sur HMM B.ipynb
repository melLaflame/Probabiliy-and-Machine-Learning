{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc6bbefa",
   "metadata": {},
   "source": [
    "## <font color=darkcyan> Probabilités numériques: Algorithme EM sur des chaines de Markov à données cachés (The Baum Welch Algorithm)</font>\n",
    "\n",
    "Le but de ce projet est d'implémenter l'algorithme EM sur un modèle probabiliste ou les données obervables suivent des lois gaussiennes sachant les données cachés et où ces données cachés sont une chaine de Markov. Notre but sera donc d'approcher les probabilités de transitions de la chaines de Markov, les moyennes et variances des loi gaussiennes selon les différents états de la chaine et enfin les probabilités de la loi initiale de la chaine.\n",
    "\n",
    "#### <font color=darkorange>Définition formelle du problème</font>\n",
    "\n",
    "Plus formellement, soit $\\left( X_k \\right)_{ 1 \\leq k \\leq n}$ une chaine de Markov discrètes à valeurs dans $\\left(\\{1,....,r\\} \\right)$ de matrice de transition $Q$ et de loi initiale $\\nu$. On considère que cette chaîne est uniquement observée au travers\n",
    "des variables $\\left(Y_k \\right)_{ 1 \\leq k \\leq n}$ indépendantes conditionnellement à $\\left( X_k \\right)_{ 1 \\leq k \\leq n}$ et telle que pour tout\n",
    "$\\ 0 \\leq l \\leq n$\n",
    ", la loi de $Y_l$ sachant $\\left( X_k \\right)_{ 1 \\leq k \\leq n}$ suit une loi $\\mathcal{N}(\\mu_{x_l}, v_{x_l})$\n",
    ".\n",
    "Le paramètre inconnu est donc ici $θ = \\{\\mu_{1}, . . . , ,\\mu_{r}, v_{1}, . . . ,v_{r}, Q, \\nu\\}$.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ddf442c",
   "metadata": {},
   "source": [
    "#### <font color=darkorange>  - Etape E de l'Algorithme </font>\n",
    "\n",
    "- Dans l'étape E de l'algorithme, le but est de calculer la quantité intermédiaire de l'EM :\n",
    "\n",
    "    $Q(\\theta, \\theta^\\prime)$ =  $\\mathbb{E}_{\\theta '}[\\log p_{\\theta}(X_{0:n}, Y_{0:n})\\mid Y_{0:n}]$ où $p_{\\theta}$ est la densité du vecteur $(X_{0},...,X_{n},Y_{1},...,Y_{0})$.\n",
    "\n",
    "    On peut commencer par écrire $\\log p_{\\theta}(X_{0:n}, Y_{0:n})$ en fonction des paramètres du problème.\n",
    "\n",
    "    On sait que $\\forall \\theta$ et $\\forall x_{0},..,x_{n},y_{0},..,y_{n} \\in \\mathbb{R}^{n+1} \\times \\{1,....,r\\}^{n+1} $, on a  $p_{\\theta}(x_{0},..,x_{n},y_{0},..,y_{n}) = p_{\\theta}(y_{0},..,y_{n} \\mid x_{0},..,x_{n})  f^X_{\\theta}(x_{0},..,x_{n})$ ou $f^X$ est la densité du vecteur $(X_{0},...,X_{n})$ .\n",
    "\n",
    "    Or sachant $(X_{0},...,X_{n})$,  $(Y_{0},...,Y_{n})$ est un vecteur de variables aléatoire indépendantes, respectivement de loi $\\mathcal{N}(\\mu_{x_k}, v_{x_k})$,  $\\forall k \\in \\{ 0, \\ldots, n \\}$.\n",
    "\n",
    "    Donc $p_{\\theta}(y_{0},..,y_{n},\\mid x_{0},..,x_{n})$ = $\\prod_{k=0}^n \\frac{1}{\\sqrt{2\\pi v_{y_i}}} \\exp\\left(-\\frac{(y_k-\\mu_{x_i})^2}{2v_{x_k}}\\right)$.\n",
    "\n",
    "    On a aussi $f^Y_{\\theta}(x_{0},..,x_{n})$ = $\\nu(x_{0})$  $\\prod_{k=0}^{n-1} q_{x_k,x_{k+1}}$\n",
    "\n",
    "    On a donc $p_{\\theta}(x_{0},..,x_{n},y_{0},..,y_{n})$ = $\\nu(x_{0})$ $\\prod_{k=0}^n \\frac{1}{\\sqrt{2\\pi v_{x_k}}} \\exp\\left(-\\frac{(y_k-\\mu_{x_k})^2}{2v_{x_k}}\\right) \\times \\prod_{k=0}^{n-1} q_{x_k,x_{k+1}} $\n",
    "\n",
    "    donc $\\log p_{\\theta}(X_{0:n}, Y_{0:n})$ = $\\log(\\nu(X_0)) + \\sum_{k=0}^n \\log\\left(\\frac{1}{\\sqrt{2\\pi v_{X_k}}}\\right) - \\sum_{k=0}^n \\frac{(Y_k - \\mu_{X_k})^2}{2v_{X_k}} + \\sum_{k=0}^{n-1} \\log(q_{X_k} q_{X_{k+1}})$\n",
    "\n",
    "    Enfin on a $Q(\\theta, \\theta^\\prime)$ =  $\\mathbb{E}_{\\theta '}[\\log p_{\\theta}(X_{0:n}, Y_{0:n})\\mid Y_{0:n} = y_{0:n} ] =  \\mathbb{E}_{\\theta '}[\\log(\\nu^{\\theta}(X_0)) + \\sum_{k=0}^n \\log\\left(\\frac{1}{\\sqrt{2\\pi X_k}}\\right) - \\sum_{k=0}^n \\frac{(y_i - \\mu^{\\theta}_{X_k})^2}{2v^{\\theta}_{X_k}} + \\sum_{k=0}^{n-1} \\log(q^{\\theta}_{X_k X_{k+1}}) \\mid Y_{0:n} = y_{0:n} ]$ = $\\mathbb{E}_{\\theta '}[\\log(\\nu^{\\theta}(X_0)) \\mid Y_{0:n} = y_{0:n}] + \\mathbb{E}_{\\theta '}[\\sum_{k=0}^n \\log\\left(\\frac{1}{\\sqrt{2\\pi \\nu^\\theta_{X_k}}}\\right) - \\sum_{k=0}^n \\frac{(y_k - \\mu^{\\theta}_{X_k})^2}{2v^{\\theta}_{X_k}}\\mid Y_{0:n} = y_{0:n}] + \\mathbb{E}_{\\theta '}[\\sum_{k=0}^{n-1} \\log(q^{\\theta}_{X_k X_{k+1}})\\mid Y_{0:n} = y_{0:n}]$ \n",
    "    $ = \\sum_{i=1}^{r} \\log(v^{\\theta}(i)) P_{\\theta '}(X_0=i \\mid Y_{0:n} = y_{0:n}) - \n",
    "     \\sum_{i=0}^r\\sum_{k=0}^n (\\log\\left(\\frac{1}{\\sqrt{2\\pi \\nu^\\theta_{i}}}\\right) - \\frac{(y_k - \\mu^{\\theta}_{i})^2}{2v^{\\theta}_{i}}) P_{\\theta '}(X_k=i \\mid Y_{0:n} = y_{0:n}) + \\sum_{i=0}^r \\sum_{j=0}^r \\sum_{k=0}^{n-1} \\log(q^{\\theta}_{i j}) P_{\\theta '}(X_k=i, X_{k+1}=j \\mid Y_{0:n} = y_{0:n}) $ \n",
    "\n",
    "    $\\underline{Conclusion}$ : \n",
    "\n",
    "    $ \\mathbb{E}_{\\theta '}[\\log p_{\\theta}(X_{0:n}, Y_{0:n})\\mid Y_{0:n} = y_{0:n} ] $\n",
    "\n",
    "    $ = \\boxed{ \\sum_{i=1}^{r} \\log(\\nu^{\\theta}(i)) \\omega^{\\theta\\prime}_{0}(i) - \n",
    "    \\sum_{i=0}^r\\sum_{k=0}^n \\left(\\log\\left(\\frac{1}{\\sqrt{2\\pi v^\\theta_{i}}}\\right) - \\frac{(y_k - \\mu^{\\theta}_{i})^2}{2v^{\\theta}_{i}} \\right) \\omega^{\\theta\\prime}_{k}(i)+ \\sum_{i=0}^r \\sum_{j=0}^r \\sum_{k=0}^{n-1} \\log(q^{\\theta}_{i j}) \\omega^{\\theta\\prime}_{k,k+1}(i,j)} $ \n",
    "\n",
    "#### <font color=darkorange>  - Etape M de l'Algorithme </font>\n",
    "\n",
    "- Dans l'étape M de l'algorithme, le but est de maximiser la quatité intermédiaire :\n",
    " \n",
    "\n",
    "    En déterminant où s'annulent les dérivées partiels et grâce aux multiplicateurs de lagranges, on obtient la mise à jour :\n",
    "\n",
    "    $\\boxed{\\forall i,j \\in \\{1,...,r\\}^2, q^\\theta_{ij} = \\frac{\\sum_{k=0}^n \\omega_{k,k+1}^{\\theta'}(i,j)}{\\sum_{k=0}^n \\omega_k^{\\theta'}(i)}}$\n",
    "\n",
    "    $ \\boxed{ \\forall i \\in \\{1,...,r\\},  \\mu^\\theta_{i} = \\frac{\\sum_{k=0}^n y_k w_{k}^{\\theta'}(i)}{\\sum_{k=0}^n w_{k}^{\\theta'}(i)}}$\n",
    "\n",
    "    $\\boxed{ \\forall i \\in \\{1,...,r\\}, v^\\theta_{i} = 2\\sum_{k=0}^n (y_k - \\mu^\\theta_i) \\omega ^{\\theta '}_{k}(i)} $\n",
    "\n",
    "    $\\boxed { \\forall i \\in \\{1,...,r\\}, \\nu^\\theta_{i} = \\omega ^{\\theta '}_{0}(i)} $\n",
    "\n",
    "    Maintenant que nous avons nos mis à jours, il nous faut des méthodes pour calculer ou approcher les probabilités $\\omega_{k,k+1}^{\\theta'}(i,j)$ et \n",
    "    $\\omega_k^{\\theta'}(i)$.\n",
    "    Il existe des algorithmes capables d'approchers ces valeurs !\n",
    "\n",
    "    $\\omega_{k}^{\\theta'}(i)$ = $P_{\\theta '}(X_k=i \\mid Y_{0:n} = y_{0:n}) = \\frac{ P_{\\theta'}(X_k = i, Y_{0:n} = y_{0:n})}{P_{\\theta'}(Y_{0:n} = y_{0:n})} $\n",
    "\n",
    "\n",
    "    $\\omega_{k,k+1}^{\\theta'}(i,j)$ = $P_{\\theta '}(X_k=i, X_{k+1}=j \\mid Y_{0:n} = y_{0:n}) = \\frac{ P_{\\theta'}(X_k = i, X_{k+1} = j, Y_{0:n} = y_{0:n})}{P_{\\theta'}(Y_{0:n} = y_{0:n})} $ \n",
    "\n",
    "    Pour estimiter la quantité $\\omega_{k}^{\\theta'}(i)$ = $P_{\\theta '}(X_k=i \\mid Y_{0:n} = y_{0:n}) = \\frac{ P_{\\theta'}(X_k = i, Y_{0:n} = y_{0:n})}{P_{\\theta'}(Y_{0:n} = y_{0:n})} $, on utilise des algorithmes récursifs appelé forward procedure et backward procedure.\n",
    "\n",
    "   On définie $\\forall n$, $\\forall i$  $\\alpha_i(t) = P(Y_1 = y_1, \\ldots, Y_t = y_t, X_t = i | \\theta)$ et $\\beta_i(n) = P(Y_{t+1} = y_1, \\ldots, Y_n = y_n, X_t = i | \\theta)$\n",
    "\n",
    "   et on a $\\forall i$ $P_{\\theta'}(X_k = i, Y_{0:n} = y_{0:n})$ = $\\alpha_i(k)$ $\\beta_i(k)$ et $P_{\\theta'}(Y_{0:n} = y_{0:n}) = \\sum_{k=0}^n \\alpha_i(k) \\beta_i(k)$ donc $\\boxed{\\omega_{k}^{\\theta'}(i) = \\frac{\\alpha_k(i) \\beta_k(i)}{\\sum_{k=0}^n \\alpha_k(i) \\beta_k(i)}}$.\n",
    "\n",
    "   Par un calcul simmilaire on obtient $\\boxed{ \\omega_{k,k+1}^{\\theta'}(i,j) = \\frac{\\alpha_i(t) a_{ij} b_j(y_{t+1}) \\beta_j(t+1)}{\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i(t) a_{ij} b_j(y_{t+1}) \\beta_j(t+1)}}$\n",
    "   \n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkorange>  - Partie Application </font>\n",
    "\n",
    "#### <font color=darkorange>  Simulations des obervations </font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf1c1d8b",
   "metadata": {},
   "source": [
    "Notre but va donc être de fixer les paramètres de $\\theta$ de notre modèle, simuler des observations de notre modèle sous ce theta fixé. Nous voulons vérifier que notre algorithme EM approxime bien notre paramètre de theta en supposant connues seulement les observations.\n",
    "On suppose connue seulement les observations des Yk et on applique l'algorithme EM pour retrouver nos paramètres initiales.\n",
    "\n",
    "- Etape 1 - Simulation d'observations d'une chaine de markov de matrice de transition Q, loi initiale v et espace d'états E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6005ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5109b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulation(Q,v,e):\n",
    "    X = np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf841d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "35445b119a28106fe48080b1533ab17d11af97993b5b943fb4e17d57e7d74253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
