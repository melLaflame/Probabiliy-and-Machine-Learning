{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc6bbefa",
   "metadata": {},
   "source": [
    "## <font color=darkcyan> Probabilités numériques: Algorithme EM sur des chaines de Markov à données cachés (The Baum Welch Algorithm)</font>\n",
    "\n",
    "Le but de ce projet est d'implémenter l'algorithme EM sur un modèle probabiliste ou les données obervables suivent des lois gaussiennes sachant les données cachés et où ces données cachés sont une chaine de Markov. Notre but sera donc d'approcher les probabilités de transitions de la chaines de Markov, les moyennes et variances des loi gaussiennes selon les différents états de la chaine et enfin les probabilités de la loi initiale de la chaine.\n",
    "\n",
    "#### <font color=darkorange>Définition formelle du problème</font>\n",
    "\n",
    "Plus formellement, soit $\\left( X_k \\right)_{ 1 \\leq k \\leq n}$ une chaine de Markov discrètes à valeurs dans $\\left(\\{1,....,r\\} \\right)$ de matrice de transition $Q$ et de loi initiale $\\nu$. On considère que cette chaîne est uniquement observée au travers\n",
    "des variables $\\left(Y_k \\right)_{ 1 \\leq k \\leq n}$ indépendantes conditionnellement à $\\left( X_k \\right)_{ 1 \\leq k \\leq n}$ et telle que pour tout\n",
    "$\\ 0 \\leq l \\leq n$\n",
    ", la loi de $Y_l$ sachant $\\left( X_k \\right)_{ 1 \\leq k \\leq n}$ suit une loi $\\mathcal{N}(\\mu_{x_l}, v_{x_l})$\n",
    ".\n",
    "Le paramètre inconnu est donc ici $θ = \\{\\mu_{1}, . . . , ,\\mu_{r}, v_{1}, . . . ,v_{r}, Q, \\nu\\}$.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ddf442c",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "#### <font color=darkorange>  - Etape E de l'Algorithme </font>\n",
    "\n",
    "- Dans l'étape E de l'algorithme, le but est de calculer la quantité intermédiaire de l'EM :\n",
    "\n",
    "    $Q(\\theta, \\theta^\\prime)$ =  $\\mathbb{E}_{\\theta '}[\\log p_{\\theta}(X_{0:n}, Y_{0:n})\\mid Y_{0:n}]$ où $p_{\\theta}$ est la densité du vecteur $(X_{0},...,X_{n},Y_{1},...,Y_{0})$.\n",
    "\n",
    "    On peut commencer par écrire $\\log p_{\\theta}(X_{0:n}, Y_{0:n})$ en fonction des paramètres du problème.\n",
    "\n",
    "    On sait que $\\forall \\theta$ et $\\forall x_{0},..,x_{n},y_{0},..,y_{n} \\in \\mathbb{R}^{n+1} \\times \\{1,....,r\\}^{n+1} $, on a  $p_{\\theta}(x_{0},..,x_{n},y_{0},..,y_{n}) = p_{\\theta}(y_{0},..,y_{n} \\mid x_{0},..,x_{n})  f^X_{\\theta}(x_{0},..,x_{n})$ ou $f^X$ est la densité du vecteur $(X_{0},...,X_{n})$ .\n",
    "\n",
    "    Or sachant $(X_{0},...,X_{n})$,  $(Y_{0},...,Y_{n})$ est un vecteur de variables aléatoire indépendantes, respectivement de loi $\\mathcal{N}(\\mu_{x_k}, v_{x_k})$,  $\\forall k \\in \\{ 0, \\ldots, n \\}$.\n",
    "\n",
    "    Donc $p_{\\theta}(y_{0},..,y_{n},\\mid x_{0},..,x_{n})$ = $\\prod_{k=0}^n \\frac{1}{\\sqrt{2\\pi v_{y_i}}} \\exp\\left(-\\frac{(y_k-\\mu_{x_i})^2}{2v_{x_k}}\\right)$.\n",
    "\n",
    "    On a aussi $f^Y_{\\theta}(x_{0},..,x_{n})$ = $\\nu(x_{0})$  $\\prod_{k=0}^{n-1} q_{x_k,x_{k+1}}$\n",
    "\n",
    "    On a donc $p_{\\theta}(x_{0},..,x_{n},y_{0},..,y_{n})$ = $\\nu(x_{0})$ $\\prod_{k=0}^n \\frac{1}{\\sqrt{2\\pi v_{x_k}}} \\exp\\left(-\\frac{(y_k-\\mu_{x_k})^2}{2v_{x_k}}\\right) \\times \\prod_{k=0}^{n-1} q_{x_k,x_{k+1}} $\n",
    "\n",
    "    donc $\\log p_{\\theta}(X_{0:n}, Y_{0:n})$ = $\\log(\\nu(X_0)) + \\sum_{k=0}^n \\log\\left(\\frac{1}{\\sqrt{2\\pi v_{X_k}}}\\right) - \\sum_{k=0}^n \\frac{(Y_k - \\mu_{X_k})^2}{2v_{X_k}} + \\sum_{k=0}^{n-1} \\log(q_{X_k} q_{X_{k+1}})$\n",
    "\n",
    "    Enfin on a $Q(\\theta, \\theta^\\prime)$ =  $\\mathbb{E}_{\\theta '}[\\log p_{\\theta}(X_{0:n}, Y_{0:n})\\mid Y_{0:n} = y_{0:n} ] =  \\mathbb{E}_{\\theta '}[\\log(\\nu^{\\theta}(X_0)) + \\sum_{k=0}^n \\log\\left(\\frac{1}{\\sqrt{2\\pi X_k}}\\right) - \\sum_{k=0}^n \\frac{(y_i - \\mu^{\\theta}_{X_k})^2}{2v^{\\theta}_{X_k}} + \\sum_{k=0}^{n-1} \\log(q^{\\theta}_{X_k X_{k+1}}) \\mid Y_{0:n} = y_{0:n} ]$ = $\\mathbb{E}_{\\theta '}[\\log(\\nu^{\\theta}(X_0)) \\mid Y_{0:n} = y_{0:n}] + \\mathbb{E}_{\\theta '}[\\sum_{k=0}^n \\log\\left(\\frac{1}{\\sqrt{2\\pi \\nu^\\theta_{X_k}}}\\right) - \\sum_{k=0}^n \\frac{(y_k - \\mu^{\\theta}_{X_k})^2}{2v^{\\theta}_{X_k}}\\mid Y_{0:n} = y_{0:n}] + \\mathbb{E}_{\\theta '}[\\sum_{k=0}^{n-1} \\log(q^{\\theta}_{X_k X_{k+1}})\\mid Y_{0:n} = y_{0:n}]$ \n",
    "    $ = \\sum_{i=1}^{r} \\log(v^{\\theta}(i)) P_{\\theta '}(X_0=i \\mid Y_{0:n} = y_{0:n}) - \n",
    "     \\sum_{i=0}^r\\sum_{k=0}^n (\\log\\left(\\frac{1}{\\sqrt{2\\pi \\nu^\\theta_{i}}}\\right) - \\frac{(y_k - \\mu^{\\theta}_{i})^2}{2v^{\\theta}_{i}}) P_{\\theta '}(X_k=i \\mid Y_{0:n} = y_{0:n}) + \\sum_{i=0}^r \\sum_{j=0}^r \\sum_{k=0}^{n-1} \\log(q^{\\theta}_{i j}) P_{\\theta '}(X_k=i, X_{k+1}=j \\mid Y_{0:n} = y_{0:n}) $ \n",
    "\n",
    "    $\\underline{Conclusion}$ : \n",
    "\n",
    "    $ \\mathbb{E}_{\\theta '}[\\log p_{\\theta}(X_{0:n}, Y_{0:n})\\mid Y_{0:n} = y_{0:n} ] $\n",
    "\n",
    "    $ = \\boxed{ \\sum_{i=1}^{r} \\log(\\nu^{\\theta}(i)) \\omega^{\\theta\\prime}_{0}(i) - \n",
    "    \\sum_{i=0}^r\\sum_{k=0}^n \\left(\\log\\left(\\frac{1}{\\sqrt{2\\pi v^\\theta_{i}}}\\right) - \\frac{(y_k - \\mu^{\\theta}_{i})^2}{2(v^{\\theta}_{i})^2} \\right) \\omega^{\\theta\\prime}_{k}(i)+ \\sum_{i=0}^r \\sum_{j=0}^r \\sum_{k=0}^{n-1} \\log(q^{\\theta}_{i j}) \\omega^{\\theta\\prime}_{k,k+1}(i,j)} $ \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35026c5f",
   "metadata": {},
   "source": [
    "\n",
    "#### <font color=darkorange>  - Etape M de l'Algorithme </font>\n",
    "\n",
    "- Dans l'étape M de l'algorithme, le but est de maximiser la quatité intermédiaire :\n",
    " \n",
    "\n",
    "    En déterminant où s'annulent les dérivées partiels et grâce aux multiplicateurs de lagranges, on obtient la mise à jour :\n",
    "\n",
    "    $\\boxed{\\forall i,j \\in \\{1,...,r\\}^2, q^\\theta_{ij} = \\frac{\\sum_{k=0}^n \\omega_{k,k+1}^{\\theta'}(i,j)}{\\sum_{k=0}^n \\omega_k^{\\theta'}(i)}}$\n",
    "\n",
    "    $ \\boxed{ \\forall i \\in \\{1,...,r\\},  \\mu^\\theta_{i} = \\frac{\\sum_{k=0}^n y_k w_{k}^{\\theta'}(i)}{\\sum_{k=0}^n w_{k}^{\\theta'}(i)}}$\n",
    "\n",
    "    $\\boxed{\\forall i \\in \\{1,...,r\\}, v^\\theta_{i} = \\frac{\\sum_{k=0}^n (y_k - \\mu^\\theta_i)^2 \\omega ^{\\theta '}_{k}(i)}{\\sum_{k=0}^n w_{k}^{\\theta'}(i)}}$\n",
    "\n",
    "    $\\boxed { \\forall i \\in \\{1,...,r\\}, \\nu^\\theta_{i} = \\omega ^{\\theta '}_{0}(i)} $\n",
    "\n",
    "    Maintenant que nous avons nos mises à jours, il nous faut des méthodes pour calculer ou approcher les probabilités $\\omega_{k,k+1}^{\\theta'}(i,j)$ et \n",
    "    $\\omega_k^{\\theta'}(i)$.\n",
    "    Il existe des algorithmes capables d'approchers ces valeurs !\n",
    "\n",
    "    $\\omega_{k}^{\\theta'}(i)$ = $P_{\\theta '}(X_k=i \\mid Y_{0:n} = y_{0:n}) = \\frac{ P_{\\theta'}(X_k = i, Y_{0:n} = y_{0:n})}{P_{\\theta'}(Y_{0:n} = y_{0:n})} $\n",
    "\n",
    "\n",
    "    $\\omega_{k,k+1}^{\\theta'}(i,j)$ = $P_{\\theta '}(X_k=i, X_{k+1}=j \\mid Y_{0:n} = y_{0:n}) = \\frac{ P_{\\theta'}(X_k = i, X_{k+1} = j, Y_{0:n} = y_{0:n})}{P_{\\theta'}(Y_{0:n} = y_{0:n})} $ \n",
    "\n",
    "    Pour estimiter la quantité Ces quantités, on utilise des algorithmes récursifs appelé forward procedure et backward procedure.\n",
    "\n",
    "   On introduit les quantités $\\forall t$, $\\forall i$  $\\alpha_i(t) = \\mathrm{P}(Y_1 = y_1, \\ldots, Y_t = y_t, X_t = i | \\theta)$ et $\\beta_i(n) = P(Y_{t+1} = y_1, \\ldots, Y_n = y_n,| X_t = i, \\theta)$ et $ b_i(y_k)  = P(Y_k = y_k | X_k = i, \\theta)$. Dans le cas discret cela fonctionnerait sauf que cette dernière quantité donne toujours 0 car les observations sont continues. Nous allons donc supposer par abus de langague que $b_i(y_k) = \\int_{y_k-\\epsilon}^{y_k+\\epsilon} f_{Y_k|X_k = i}(y) \\,dy$ pour epsilon assez petit envirion $\\epsilon = 0.01$ .\n",
    "\n",
    "   et on obtient finalement:\n",
    "\n",
    "   $\\boxed{\\omega_{k}^{\\theta'}(i) = \\frac{\\alpha_i(k) \\beta_i(k)}{\\sum_{j=0}^r \\alpha_j(k) \\beta_j(k)}}$.\n",
    "\n",
    "   $\\boxed{ \\omega_{k,k+1}^{\\theta'}(i,j) = \\frac{\\alpha_i(k) a_{ij} b_{{j}}(y_{k+1}) \\beta_{j}(k+1)}{\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i(k) a_{ij}  b_{{j}}(y_{k+1}) \\beta_{j}(k+1)}}$\n",
    "\n",
    "   Pour calculer les quantité $\\alpha_i(k)$ et  $\\beta_i(k)$, on utilise les algorithmes récursif forward et backward suivant:\n",
    "\n",
    "   Initalisation:\n",
    "\n",
    "   $\\alpha_i(1) = \\pi_i b_i(y_1)$\n",
    "\n",
    "   Récursion:\n",
    "\n",
    "   $\\alpha_{i}(k+1) = \\left[\\sum_{j=1}^r \\alpha_{j}(k)a_{ij}\\right]b_i(y_{k+1})$\n",
    "\n",
    "   Initalisation:\n",
    "\n",
    "   $\\beta_i(n) = 1 $\n",
    "\n",
    "   Récursion:\n",
    "\n",
    "   $\\beta_i(k) = \\sum_{j=1}^r a_{ij}b_j(y_{k+1})\\beta_{j}(k+1)$\n",
    "\n",
    "   On pose la matrice alpha de taille n*r tel que a_k_i = alpha_i(k)\n",
    "   et la matrice b de taille n * r tel que b_k_i = b_i(k)\n",
    "\n",
    "   On a besoin de cette initalisation pour que la récursion soit juste. \n",
    "   Ces récursions sont logique... en effet...\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkorange>  - Partie application et execution de l'algorithme EM </font>\n",
    "\n",
    "#### <font color=darkorange>  Simulations des obervations </font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf1c1d8b",
   "metadata": {},
   "source": [
    "Notre but va donc être de fixer les paramètres de $\\theta$ de notre modèle, simuler des observations de notre modèle sous ce theta fixé. Nous voulons vérifier que notre algorithme EM approxime bien notre paramètre de theta en supposant connues seulement les observations.\n",
    "On suppose connue seulement les observations des Yk et on applique l'algorithme EM pour retrouver nos paramètres initiales.\n",
    "\n",
    "- Etape 1 - Simulation d'observations d'une chaine de markov de matrice de transition Q, loi initiale v et espace d'états E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "6005ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "2a5109b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulation(Q,nu,e,n_steps): \n",
    "    # Q : matrice de transition (array de float dimension  r * r) \n",
    "    # nu :  loi initiale (array de float dimension r) \n",
    "    # e : liste_d'etats (array de int dimension r)\n",
    "    #n_steps : nombre de pas de la chaines à simuler (int)\n",
    "\n",
    "    X = np.zeros(n_steps,dtype = np.int64)\n",
    "    current_state = rng.choice(e,p = nu)\n",
    "    for i in range(n_steps): \n",
    "        X[i] = current_state\n",
    "        current_state = rng.choice(e,p = Q[current_state - 1])\n",
    "    return X\n",
    "\n",
    "Q = np.array([[1/2,1/2],[1/2,1/2]])\n",
    "nu = np.array([1/2,1/2])\n",
    "E = np.array([1,2])\n",
    "n_steps = 3\n",
    "X = Simulation(Q,nu,E,n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "edaf841d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.20027678 1.83190726 3.97066984]\n"
     ]
    }
   ],
   "source": [
    "def Simulation_Observation(chaine_states,means,vars):  \n",
    "    # chaine_states -> etats de la chaine pour simuler les observations -> array de int\n",
    "    # means - > liste des mean_x (float)\n",
    "    # var -> liste des var_x (float)\n",
    "    Y = np.zeros(chaine_states.size,dtype = np.float64)\n",
    "    for i in range(chaine_states.size): \n",
    "        normale_i = stats.norm(means[chaine_states[i] - 1],vars[chaine_states[i] - 1])\n",
    "        Y[i] = normale_i.rvs()\n",
    "    return Y\n",
    "\n",
    "means = np.array([0,2])\n",
    "vars = np.array([1,1])\n",
    "Y = Simulation_Observation(X,means,vars)\n",
    "print(Y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "982d881d",
   "metadata": {},
   "source": [
    "Ici on a considére une chaine de markov de matrice de transion ... et d'états .... et de loi initiale .... et des moyennes .... et des variances....\n",
    "Maintenant on ne regarde plus que les observations Yk et on applique l'algorithme en choisissant comme theta initiale, $\\theta$ = .... .\n",
    "Mise à jour\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5698c61a",
   "metadata": {},
   "source": [
    "#### <font color=darkorange>  Calcul des omegas </font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "b2ef01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.01\n",
    "#Y = np.array([-1.06762504, 0.36554341, -0.23488898, 1.7271385, -0.04636244])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d89cc61",
   "metadata": {},
   "source": [
    "- calcul de b_j_yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "5450b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0309195  0.30110577]\n",
      " [0.9671279  0.61012643]\n",
      " [0.0019526  0.0887678 ]]\n"
     ]
    }
   ],
   "source": [
    "def create_B(Y,means,vars):\n",
    "    n = Y.size\n",
    "    r = means.size \n",
    "    B = np.zeros([n,r],dtype = np.float64)\n",
    "    for i in range(r):\n",
    "        normal_law = stats.norm(loc = means[i], scale = np.sqrt(vars[i]))\n",
    "        B[:,i] = normal_law.pdf(Y)\n",
    "        B[:,i] = B[:,i] / np.sum(B[:,i])\n",
    "    return B\n",
    "\n",
    "B = create_B(Y,means,vars)\n",
    "print(B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c69fdfd",
   "metadata": {},
   "source": [
    "- calcul de a_k_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "f2571617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.54597509e-02 1.50552886e-01]\n",
      " [8.02777261e-02 5.06443486e-02]\n",
      " [1.27819417e-04 5.81083225e-03]]\n"
     ]
    }
   ],
   "source": [
    "def create_alpha (Y,Q,nu,means,vars):\n",
    "    n = Y.size\n",
    "    r = means.size \n",
    "    B = create_B(Y,means,vars)\n",
    "    A = np.zeros([n,r])\n",
    "    A[0,:] = nu * B[0,:]\n",
    "    for k in range(1,n):\n",
    "        for i in range(r):\n",
    "            A[k,i] = (A[k-1,:] @ Q[:,i]) * B[k,i]\n",
    "    return A\n",
    "\n",
    "print(create_alpha(Y,Q,nu,means,vars))\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "958f0ec4",
   "metadata": {},
   "source": [
    "- calcul de b_k_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "d6d2b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03577229 0.03577229]\n",
      " [0.0453602  0.0453602 ]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def create_beta(Y,Q,nu,means,vars):\n",
    "    n = Y.size\n",
    "    r = nu.size \n",
    "    B = create_B(Y,means,vars)    \n",
    "    beta = np.zeros([n,r])\n",
    "    beta[-1,:] = np.ones(r)\n",
    "\n",
    "    for k in range(n-2, -1, -1):\n",
    "        for i in range(r):\n",
    "            beta[k,i] = np.sum(beta[k+1,:] * B[k+1,:] * Q[i,:])\n",
    "\n",
    "    return beta\n",
    "\n",
    "print(create_beta(Y,Q,nu,means,vars))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "682f1a6e",
   "metadata": {},
   "source": [
    "- mis à jour loi initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "14b5708b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1312751529.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[493], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def omega(k,i):\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def omega_now(Y,Q,nu,means,vars):    \n",
    "    n = Y.size\n",
    "    r = nu.size \n",
    "    omega = np.zeros([n,r])\n",
    "    alpha = create_alpha(Y,Q,nu,means,vars)\n",
    "    beta = create_beta(Y,Q,nu,means,vars)\n",
    "    for k in range(n):\n",
    "        for i in range(r):\n",
    "            omega[k,i] = (alpha[k,i] * beta[k,i]) / np.sum(alpha[k:,] * beta[k:,])\n",
    "\n",
    "    return omega\n",
    "\n",
    "def omega_next(Y,Q,nu,means,vars):    \n",
    "    n = Y.size\n",
    "    r = nu.size \n",
    "    omega = np.zeros([n,r])\n",
    "    alpha = create_alpha(Y,Q,nu,means,vars)\n",
    "    beta = create_beta(Y,Q,nu,means,vars)\n",
    "    for k in range(n):\n",
    "        for i in range(r):\n",
    "            omega[k,i] = (alpha[k,i] * beta[k,i]) / np.sum(alpha[k:,] * beta[k:,])\n",
    "\n",
    "    return omega\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "35445b119a28106fe48080b1533ab17d11af97993b5b943fb4e17d57e7d74253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
